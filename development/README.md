# Tinker
## _Гибкий, готовый двигаться: мультяшный двуногий робот!_ 

Тинкер шагает с помощью RL (обучение с подкреплением), может стабильно стоять и ходить. Во время тестирования потребуется программное 
обеспечние для обучения нейронной сети. 

## Благодарности
Этот проект явяется воспроизведением робота  "[Тинкер](https://github.com/Yuexuan9/Tinker)"
Оригинальный проект основан на коде [LocomotionWithNP3O](https://github.com/zeonsunlightyu/LocomotionWithNP3O).

С работой реального робота можно ознакомиться по ссылке ниже:
[TODO: ссылка на работу робота](https://b23.tv/haLM4jg)

## Определение систему координат
Во-первых, экспортируйте URDF и используйте RViz для просмотра системы координат и углов, в основном для увеличения пределов соединения, угловых скоростей и максимального крутящего момента. Файлы URDF и XML для Tinker выглядят следующим образом:

Вы можете скачать файл с [Tinker.zip](https://drive.google.com/file/d/1ZrNIlMniP54uTsEq2xIo4VjpPapP6Isk/view).

Для просмотра URDF в Rviz, введите следующую команду:
```bash
roslaunch urdf_tutorial display.launch model:=/home/pi/Downloads/LocomotionWithNP3O-master/resources//tinker/urdf/tinker_urdf.urdf
```
<div align="center">
<img src="https://github.com/Yuexuan9/Tinker/raw/main/docs/images/development/1.PNG" height="300" />
<img src="https://github.com/Yuexuan9/Tinker/raw/main/docs/images/development/2.PNG" height="300" />
</div>

## Настройка среды
Параметры системы:

>Версия ОС: Ubuntu 24.04.2 LTS  
>Видеокарта: RTX 4070 (Driver: 550.144.03, CUDA: 12.4)  
>Conda 24.9.2

Введите эти команды для настройки среды, для установки IsaacGym в виртуальную среду следуйте инструкциям из интернета:

1) Создание виртуальной среды и ее запуск:
```bash
conda create -n Tinker python=3.8.10
conda activate Tinker
```

2) Установка необходимых зависимостей:
```bash
pip install torchvision -i https://pypi.tuna.tsinghua.edu.cn/simple
pip install torch -i https://pypi.tuna.tsinghua.edu.cn/simple
pip install pyquaternion -i https://pypi.tuna.tsinghua.edu.cn/simple
pip install pyyaml -i https://pypi.tuna.tsinghua.edu.cn/simple
pip install rospkg -i https://pypi.tuna.tsinghua.edu.cn/simple
pip install pexpect -i https://pypi.tuna.tsinghua.edu.cn/simple
pip install mujoco==2.3.7 -i https://pypi.tuna.tsinghua.edu.cn/simple
pip install mujoco-py -i https://pypi.tuna.tsinghua.edu.cn/simple
pip install mujoco-python-viewer -i https://pypi.tuna.tsinghua.edu.cn/simple
pip install dm_control==1.0.14 -i https://pypi.tuna.tsinghua.edu.cn/simple
pip install opencv-python -i https://pypi.tuna.tsinghua.edu.cn/simple
pip install matplotlib -i https://pypi.tuna.tsinghua.edu.cn/simple
pip install einops -i https://pypi.tuna.tsinghua.edu.cn/simple
pip install tqdm -i https://pypi.tuna.tsinghua.edu.cn/simple
pip install packaging -i https://pypi.tuna.tsinghua.edu.cn/simple
pip install h5py -i https://pypi.tuna.tsinghua.edu.cn/simple
pip install ipython -i https://pypi.tuna.tsinghua.edu.cn/simple
pip install getkey -i https://pypi.tuna.tsinghua.edu.cn/simple
pip install wandb -i https://pypi.tuna.tsinghua.edu.cn/simple
pip install chardet -i https://pypi.tuna.tsinghua.edu.cn/simple
pip install numpy==1.23.2 -i https://pypi.tuna.tsinghua.edu.cn/simple
pip install h5py_cache -i https://pypi.tuna.tsinghua.edu.cn/simple
pip install opencv-python -i https://pypi.tuna.tsinghua.edu.cn/simple
pip install tensorboard -i https://pypi.tuna.tsinghua.edu.cn/simple
pip install lcm -i https://pypi.tuna.tsinghua.edu.cn/simple
```
3) Установка IsaacGym (выполняется при активной виртуальной среде)


Перейти на [сайт](https://developer.nvidia.com/isaac-gym)

<img src="development/images/gym1.png" height="300" />
</div>

Скачать архив:

<img src="development/images/gym2.png" height="300" />
</div>

Распаковать архив, папку `isaacgym` переместить в место хранения

Скачать зависимости:
```bash
cd isaccgym/python && pip install -e .
```

Запустить тестовый файл:

```bash
cd isaccgym/python/examples && python joint_monkey.py.
```

>! Важно запускать файл `joint_monkey.py`, находясь в папке *examples*

<img src="development/images/gym3.png" height="300" />
</div>

Добавить шорткаты в `.bashrc` для обновления переменных среды:
```bash
alias sd="source devel/setup.bash"
alias ss="source ~/.bashrc"
alias a1="conda activate Tinker"
```

## Software Usage

### Train the Model:
After setting up the environment and activating the virtual space, run the script for training:
```bash
python train.py
```
The trained model will be updated here:  
<div align="center">
<img src="https://github.com/Yuexuan9/Tinker/raw/main/docs/images/development/3.png" height="300" />
</div>

To test the model, first modify the model file used in `simple_play.py`:
```python
model_dict = torch.load(os.path.join(ROOT_DIR, '/home/pi/Downloads/back_good/LocomotionWithNP3O-masteroldx/logs/rough_go2_constraint/Oct09_10-35-52_test_barlowtwins/model_3000.pt'))
```
Then run the script:
```bash
python simple_play.py
```

### Sim2Sim Testing
After training with Isaac, the model must be transferred. Sim2Sim involves building a framework on the controller side to run the neural network and feed the necessary feedback data. Using the Human-Gym as a reference, Mujoco simulator enables Sim2Sim transfer. For real-world transfer, additional software in C++ may be required. Typical transfer frameworks can be:
<div align="center">
<img src="development/images/gym1.png" height="300" />
</div>

- **a. Using Mujoco Python/C++ simulation**: Embedding the network I/O in the Mujoco interface, preferred for semi-physical transfer.
- **b. Using Mujoco C++ simulation with LCM/ROS**: Running the Mujoco simulation asynchronously, interacting with C++ and Pytorch via LCM, suited for real-world transfer.

Before transfer, ensure the `default_dof_pos` matches the Isaac simulation:
```python
default_dof_pos = [0.0,-0.0,0.56,-1.12,0.57,0.0,0.0,0.56,-1.12,0.57]
```
Match the model path:
```bash
parser.add_argument('--load_model', type=str, default='/home/pi/Downloads/LocomotionWithNP3O-masteroldx/modelt.pt')
```
Update KP and KD to match training:
```python
class robot_config:
    kp_all = 9.0
    kd_all = 0.6
    kps = np.array([kp_all]*10, dtype=np.double)
    kds = np.array([kd_all]*10, dtype=np.double)
    tau_limit = 12. * np.ones(10, dtype=np.double)
```
Then run the transfer script:
```bash
python sim2sim_tinker.py
```

### Prototype Inference Test
For prototype testing, connect the robot to the training server via Ethernet, or deploy the model on Jetson Nano for edge deployment.
<div align="center">
<img src="https://github.com/Yuexuan9/Tinker/raw/main/docs/images/development/5.png" height="150" />
</div>
First, compile the `sim2sim_lcm` build folder, ensuring that **libtorch** and **cuDNN** are installed. Then, modify the `udp_publisher_tinker.cpp` to set the robot's IP address:
```cpp
string UDP_IP="192.168.1.242";
int SERV_PORT= 10000;
```
Modify the model path:
```cpp
model_path = "/home/pi/Downloads/back_good/LocomotionWithNP3O-master/model_jitt.pt";
load_policy();
```
Run the publisher:
```bash
./udp_publisher_tinker
```

### Robot Operation
After completing Model Training, robot installation and assembly, and migrating software package compilation, you can start testing the gait. First, select an IP name in the image file txt document of the host computer and modify the address to the master IP. Then, select the corresponding name from the drop-down connection menu of the host computer.
<div align="center">
<img src="https://github.com/Yuexuan9/Tinker/raw/main/docs/images/development/6.png" height="150" />
</div>
Before opening the host computer, insert the USB handle, connect to WIFI, and select the corresponding IP.
<div align="center">
<img src="https://github.com/Yuexuan9/Tinker/raw/main/docs/images/development/7.png" height="150" />
</div>
If the angle displayed by the host computer is 333, the controller software has already run.
<div align="center">
<img src="https://github.com/Yuexuan9/Tinker/raw/main/docs/images/development/8.png" height="300" />
</div>
If you press the reset button on the main control STM32 carrier board to reset, the above joint angle and posture of the upper computer will have normal data.
<div align="center">
<img src="https://github.com/Yuexuan9/Tinker/raw/main/docs/images/development/9.png" height="300" />
</div>
Power on:
Before powering on, it is necessary to ensure that the robot joints have been calibrated according to the assembly instructions. Lift the robot with both legs in a vertical position, long press the X button, and the robot will retract its legs to a squatting angle. At this time, the robot should be able to be placed normally on the ground to achieve locked standing.
If there is an abnormality and the motor rotates, press the button ↓ to turn off the power
Start RL program:
Start the migration software on the server or JetsonNano side, and confirm that the data is refreshed
Press X again in standing mode, and the robot will use RL data for gait driving. At this time, the left joystick corresponds to the XY speed command, the left and right triggers at the back of the handle correspond to the heading command, and the handle and joystick correspond to the robot's head posture. Press B to re-enter the locked state to achieve standing, and the power-off protection is under the button

# V1.1 Version Update Notes

## Updates
- Added gait movement and standing functions.
- Enabled quick configuration through a unified `global_config.py` file.


---


# 4. Software Usage

## 4.1 V1.1 Version Configuration

Modify the `global_config.py` file to quickly configure the training object and model files:

```python
import os
ROOT_DIR = os.path.dirname(__file__)
ENVS_DIR = os.path.join(ROOT_DIR, 'Env')

# Select robot: Taitan / Tinker / Tinymal
ROBOT_SEL = 'Tinker'

# Select gait: Trot / Stand
GAIT_SEL = 'Trot'

# Model file path
PLAY_DIR = 'XXX.pt'  

# Sim2Sim commands
SPD_X = 0.3
SPD_Y = 0.0
SPD_YAW = 0

# Training parameters
MAX_ITER = 30000
SAVE_DIV = 5000
```


## 4.2 Common Script Files

| File Name           | Function                                  |
|---------------------|-------------------------------------------|
| `train.py`          | Train the model                           |
| `play.py`           | Test the model and export `.pt` file      |
| `sim2sim_tinker.py` | Mujoco simulation testing                 |
| `pt2tvm.py`         | Convert model format to TVM for robots    |
| `exp_draw.py`       | Plot reward curves and tuning parameters  |

<div align="center">
<img src="https://github.com/Yuexuan9/Tinker/raw/main/docs/images/development/2025040401.png" height="300" />
</div>

## 4.3 Deployment Instructions

After completing the robot training and model export:

1. Convert the model to TVM format (`policy_arm64_cpu.so`)
2. Use WinSCP or other tools to upload it to the Model directory in the robot's control files
3. Ensure the file name matches the exported model

## 4.4 Development Process

<div align="center">
<img src="https://github.com/Yuexuan9/Tinker/raw/main/docs/images/development/whiteboard_exported_image.png" height="100" />
</div>

## 4.5 Tinker Robot Model Folder Structure

The following directories contain different gait models:

| Directory | Function                  |
|-----------|---------------------------|
| `Trot`    | Gait movement TVM model   |
| `Stand`   | Standing TVM model        |
| `Dance`   | Dance TVM model           |
```
